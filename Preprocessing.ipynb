{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3c804a",
   "metadata": {},
   "source": [
    "# Libraries / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc613548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a329e80",
   "metadata": {},
   "source": [
    "# Class Artstyle\n",
    " Using Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9294e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtStyleDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, mode='train', label_to_idx=None):\n",
    "        self.data_dir = Path(data_dir) / mode\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.label_to_idx = label_to_idx  # przekazana z zewnątrz\n",
    "\n",
    "        if self.label_to_idx is None:\n",
    "            raise ValueError(\"label_to_idx musi być przekazane z zewnątrz, aby zapewnić spójność klas.\")\n",
    "\n",
    "        for style_dir in self.data_dir.iterdir():\n",
    "            if style_dir.is_dir():\n",
    "                style_name = style_dir.name\n",
    "\n",
    "                if style_name not in self.label_to_idx:\n",
    "                    print(f\"⚠️ Uwaga: styl '{style_name}' nie znajduje się w label_to_idx. Pomijam.\")\n",
    "                    continue\n",
    "\n",
    "                label_idx = self.label_to_idx[style_name]\n",
    "                for img_path in style_dir.glob('*.jpg'):\n",
    "                    self.images.append(str(img_path))\n",
    "                    self.labels.append(label_idx)\n",
    "\n",
    "        self.idx_to_label = {v: k for k, v in self.label_to_idx.items()}\n",
    "        print(f\"[{mode}] Znaleziono {len(self.images)} obrazów w {len(self.label_to_idx)} klasach.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise ValueError(\"Nie można wczytać obrazu\")\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Błąd przetwarzania {img_path}: {e}\")\n",
    "            img = torch.zeros(3, 224, 224)\n",
    "            label = 0\n",
    "\n",
    "        return img, label, img_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0980c5f3",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4eda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(input_dir, output_dir, target_size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Przetwarza obrazy z folderu wejściowego i zapisuje je w folderze wyjściowym\n",
    "    z zachowaniem struktury katalogów (stylu artystycznego).\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    style_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    # foldery podfolderow sama nie wiem juz ile tych folderow\n",
    "    for style_dir in style_dirs:\n",
    "        style_name = style_dir.name\n",
    "        output_style_dir = output_path / style_name\n",
    "        output_style_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        sub_dirs = [d for d in style_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if sub_dirs:\n",
    "            for sub_dir in sub_dirs:\n",
    "                image_files = list(sub_dir.glob('*.jpg')) \n",
    "                print(f\"Przetwarzanie {len(image_files)} obrazów w stylu {style_name}\")\n",
    "                \n",
    "                for img_path in tqdm(image_files, desc=f\"Styl: {style_name}\"):\n",
    "                    process_image(img_path, output_style_dir, target_size)\n",
    "        else:\n",
    "            image_files = list(style_dir.glob('*.jpg'))\n",
    "            print(f\"Przetwarzanie {len(image_files)} obrazów w stylu {style_name}\")\n",
    "            \n",
    "            for img_path in tqdm(image_files, desc=f\"Styl: {style_name}\"):\n",
    "                process_image(img_path, output_style_dir, target_size)\n",
    "    \n",
    "    print(\"Zakończono preprocessing obrazów!\")\n",
    "\n",
    "def process_image(img_path, output_dir, target_size):\n",
    "    \"\"\"\n",
    "    Przetwarza pojedynczy obraz.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            print(f\"Nie udało się wczytać: {img_path}\")\n",
    "            return\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  #BGR na RGB\n",
    "        \n",
    "        h, w = img.shape[:2]\n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        if aspect_ratio > 3 or aspect_ratio < 1/3: \n",
    "            if w > h:\n",
    "                new_w = int(h * 1.5)  \n",
    "                start_x = int((w - new_w) / 2)\n",
    "                img = img[:, start_x:start_x+new_w]\n",
    "            else:\n",
    "                new_h = int(w * 1.5) \n",
    "                start_y = int((h - new_h) / 2)\n",
    "                img = img[start_y:start_y+new_h, :]\n",
    "        \n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        if w > h:\n",
    "            new_w = min(w, target_size[0])\n",
    "            new_h = int(h * (new_w / w))\n",
    "        else:\n",
    "            new_h = min(h, target_size[1])\n",
    "            new_w = int(w * (new_h / h))\n",
    "        \n",
    "        img_resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # padding\n",
    "        padded_img = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)\n",
    "        \n",
    "        start_x = (target_size[0] - new_w) // 2\n",
    "        start_y = (target_size[1] - new_h) // 2\n",
    "        padded_img[start_y:start_y+new_h, start_x:start_x+new_w] = img_resized\n",
    "        \n",
    "        output_file = output_dir / img_path.name\n",
    "        cv2.imwrite(str(output_file), cv2.cvtColor(padded_img, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Błąd podczas przetwarzania {img_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5e358",
   "metadata": {},
   "source": [
    "# Datasplitting\n",
    "train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9097886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_with_balancing(processed_dir, output_dir, \n",
    "                             train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, \n",
    "                             min_class_size=2000, max_class_size=3000,\n",
    "                             random_state=42):\n",
    "   \n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Proporcje podziału muszą sumować się do 1.0\"\n",
    "    \n",
    "    processed_path = Path(processed_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Tymczasowy katalog dla zbalansowanych danych\n",
    "    balanced_path = output_path / \"balanced_temp\"\n",
    "    if balanced_path.exists():\n",
    "        shutil.rmtree(balanced_path)\n",
    "    balanced_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Katalogi dla zbiorów finalnych\n",
    "    train_dir = output_path / \"train\"\n",
    "    val_dir = output_path / \"val\"\n",
    "    test_dir = output_path / \"test\"\n",
    "    \n",
    "    for directory in [train_dir, val_dir, test_dir]:\n",
    "        if directory.exists():\n",
    "            shutil.rmtree(directory)\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    style_dirs = [d for d in processed_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(\"Rozpoczynam balansowanie klas...\")\n",
    "    \n",
    "    balanced_files = {}\n",
    "    \n",
    "    # ETAP 1: Balansowanie każdej klasy\n",
    "    for style_dir in style_dirs:\n",
    "        style_name = style_dir.name\n",
    "        print(f\"\\nPrzetwarzanie stylu: {style_name}\")\n",
    "        \n",
    "        balanced_style_dir = balanced_path / style_name\n",
    "        balanced_style_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        image_files = list(style_dir.glob('*.jpg'))\n",
    "        total_images = len(image_files)\n",
    "        print(f\"Znaleziono {total_images} oryginalnych obrazów\")\n",
    "        \n",
    "        balanced_files[style_name] = []\n",
    "        \n",
    "        # 1. Redukcja \n",
    "        if total_images > max_class_size:\n",
    "            print(f\"Redukcja liczby obrazów z {total_images} do {max_class_size}\")\n",
    "            random.seed(random_state)\n",
    "            selected_files = random.sample(image_files, max_class_size)\n",
    "            \n",
    "            for src_file in selected_files:\n",
    "                dst_file = balanced_style_dir / src_file.name\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "                balanced_files[style_name].append(dst_file)\n",
    "                \n",
    "        # 2. Oversamplig dla małych klas\n",
    "        elif total_images < min_class_size:\n",
    "            print(f\"Klasa {style_name} ma {total_images} obrazów - mniej niż {min_class_size}. Rozpoczynam uproszczony oversamplig.\")\n",
    "            \n",
    "            for src_file in image_files:\n",
    "                dst_file = balanced_style_dir / src_file.name\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "                balanced_files[style_name].append(dst_file)\n",
    "            \n",
    "            oversample_needed = min_class_size - total_images\n",
    "            \n",
    "            copies_per_image = oversample_needed // total_images + 1\n",
    "            \n",
    "            rotation_angles = [-5, -3, 3, 5]  # kąty obrotu\n",
    "            # brightness_adjustments = [-25, -15, 15, 25] \n",
    "            \n",
    "            augmented_count = 0\n",
    "            for src_file in tqdm(image_files, desc=\"Augmentacja obrazów\"):\n",
    "                for angle in rotation_angles:\n",
    "                    if augmented_count >= oversample_needed:\n",
    "                        break\n",
    "\n",
    "                    img = cv2.imread(str(src_file))\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    h, w = img.shape[:2]\n",
    "                    center = (w // 2, h // 2)\n",
    "                    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "                    rotated = cv2.warpAffine(img, rotation_matrix, (w, h))\n",
    "\n",
    "                    augmented_name = f\"{src_file.stem}_aug_r{angle}{src_file.suffix}\"\n",
    "                    augmented_path = balanced_style_dir / augmented_name\n",
    "                    cv2.imwrite(str(augmented_path), cv2.cvtColor(rotated, cv2.COLOR_RGB2BGR))\n",
    "                    balanced_files[style_name].append(augmented_path)\n",
    "                    augmented_count += 1\n",
    "\n",
    "            \n",
    "            print(f\"Utworzono {augmented_count} augmentowanych obrazów\")\n",
    "            \n",
    "        # 3. Odpowiednia wielkość (kopiuje i tyle)\n",
    "        else:\n",
    "            print(f\"Klasa {style_name} ma odpowiednią liczbę obrazów ({total_images}). Kopiowanie bez zmian.\")\n",
    "            for src_file in image_files:\n",
    "                dst_file = balanced_style_dir / src_file.name\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "                balanced_files[style_name].append(dst_file)\n",
    "    \n",
    "    print(\"\\nZakończono balansowanie klas. Dzielenie na zbiory...\")\n",
    "    \n",
    "    # Stats\n",
    "    dataset_stats = {\n",
    "        'train': Counter(),\n",
    "        'val': Counter(),\n",
    "        'test': Counter()\n",
    "    }\n",
    "    \n",
    "    # ETAP 2: Podział na zbiory treningowy, walidacyjny i testowy\n",
    "    for style_name, files in balanced_files.items():\n",
    "        print(f\"Dzielenie klasy {style_name} na zbiory...\")\n",
    "        \n",
    "        (train_dir / style_name).mkdir(exist_ok=True)\n",
    "        (val_dir / style_name).mkdir(exist_ok=True)\n",
    "        (test_dir / style_name).mkdir(exist_ok=True)\n",
    "        \n",
    "        random.seed(random_state)\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        train_size = int(len(files) * train_ratio)\n",
    "        val_size = int(len(files) * val_ratio)\n",
    "        \n",
    "        train_files = files[:train_size]\n",
    "        val_files = files[train_size:train_size+val_size]\n",
    "        test_files = files[train_size+val_size:]\n",
    "        \n",
    "        for src_file in train_files:\n",
    "            dst_file = train_dir / style_name / src_file.name\n",
    "            shutil.copy2(src_file, dst_file)\n",
    "        \n",
    "        for src_file in val_files:\n",
    "            dst_file = val_dir / style_name / src_file.name\n",
    "            shutil.copy2(src_file, dst_file)\n",
    "        \n",
    "        for src_file in test_files:\n",
    "            dst_file = test_dir / style_name / src_file.name\n",
    "            shutil.copy2(src_file, dst_file)\n",
    "        \n",
    "        # Aktualizuj statystyki\n",
    "        dataset_stats['train'][style_name] = len(train_files)\n",
    "        dataset_stats['val'][style_name] = len(val_files)\n",
    "        dataset_stats['test'][style_name] = len(test_files)\n",
    "        \n",
    "        print(f\"  - Zbiór treningowy: {dataset_stats['train'][style_name]} obrazów\")\n",
    "        print(f\"  - Zbiór walidacyjny: {dataset_stats['val'][style_name]} obrazów\")\n",
    "        print(f\"  - Zbiór testowy: {dataset_stats['test'][style_name]} obrazów\")\n",
    "    \n",
    "    shutil.rmtree(balanced_path)\n",
    "    \n",
    "    print(\"\\nZakończono podział danych na zbiory!\")\n",
    "    return dataset_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d28af",
   "metadata": {},
   "source": [
    "# Data distribution between categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989134ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(dataset_stats):\n",
    "    \"\"\"\n",
    "    Tworzy wykresy kołowe przedstawiające rozkład klas w każdym ze zbiorów.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, dataset_type in enumerate(['train', 'val', 'test']):\n",
    "        stats = dataset_stats[dataset_type]\n",
    "        labels = list(stats.keys())\n",
    "        values = list(stats.values())\n",
    "        \n",
    "        axs[i].pie(values, labels=None, autopct='%1.1f%%', startangle=90)\n",
    "        axs[i].set_title(f'Rozkład klas w zbiorze {dataset_type}')\n",
    "    \n",
    "    # Dodaj wspólną legendę\n",
    "    fig.legend(labels, loc='lower center', ncol=min(5, len(labels)))\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Tworzymy też wykres słupkowy dla lepszego porównania\n",
    "    df = pd.DataFrame(dataset_stats)\n",
    "    df.plot(kind='bar', figsize=(12, 6))\n",
    "    plt.title('Liczba obrazów w każdej klasie według zbioru')\n",
    "    plt.xlabel('Klasa')\n",
    "    plt.ylabel('Liczba obrazów')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distribution_bar.png')\n",
    "    plt.show()\n",
    "\n",
    "def denormalize(img_tensor, mean, std):\n",
    "    \"\"\"\n",
    "    Odwraca normalizację obrazu (tensor PyTorch) do zakresu [0,1].\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "    std = torch.tensor(std).view(-1, 1, 1)\n",
    "    return img_tensor * std + mean\n",
    "\n",
    "def plot_class_examples(dataset, num_examples=5):\n",
    "    \"\"\"\n",
    "    Wyświetla przykładowe obrazy z każdej klasy z oryginalnymi kolorami.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Grupuj indeksy według klas\n",
    "    class_indices = {}\n",
    "    for idx, (_, label, _) in enumerate(dataset):\n",
    "        if label not in class_indices:\n",
    "            class_indices[label] = []\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Dla każdej klasy wyświetl przykłady\n",
    "    for label, indices in class_indices.items():\n",
    "        class_name = dataset.idx_to_label[label]\n",
    "        \n",
    "        sample_indices = random.sample(indices, min(num_examples, len(indices)))\n",
    "        \n",
    "        fig, axs = plt.subplots(1, len(sample_indices), figsize=(15, 3))\n",
    "        fig.suptitle(f'Przykłady klasy: {class_name}')\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            img, _, img_path = dataset[idx]\n",
    "            \n",
    "            # Konwersja z PyTorch tensor do formatu odpowiedniego dla matplotlib\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                if img.dim() == 3 and img.shape[0] in [1, 3, 4]:  # jeśli [C, H, W]\n",
    "                    img = denormalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "                    img = np.clip(img, 0, 1)  # żeby wartości nie wyszły poza zakres [0, 1]\n",
    "                    \n",
    "                    if img.shape[2] == 1:\n",
    "                        img = img.squeeze(2)\n",
    "                else:\n",
    "                    img = img.cpu().numpy()\n",
    "            \n",
    "            if len(sample_indices) == 1:\n",
    "                axs.imshow(img)\n",
    "                axs.set_title(Path(img_path).name)\n",
    "                axs.axis('off')\n",
    "            else:\n",
    "                axs[i].imshow(img)\n",
    "                axs[i].set_title(Path(img_path).name)\n",
    "                axs[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd87b84",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2c5ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(data_dir, batch_size=32, img_size=224, num_workers=4):\n",
    "    def get_label_to_idx(data_dir):\n",
    "        train_dir = Path(data_dir) / 'train'\n",
    "        labels = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
    "        return {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    # Spójna mapa etykiet\n",
    "    label_to_idx = get_label_to_idx(data_dir)\n",
    "\n",
    "    # Transformacje\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(img_size + 32),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(img_size + 32),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Datasety z tą samą mapą etykiet\n",
    "    train_dataset = ArtStyleDataset(data_dir, transform=train_transform, mode='train', label_to_idx=label_to_idx)\n",
    "    val_dataset = ArtStyleDataset(data_dir, transform=val_test_transform, mode='val', label_to_idx=label_to_idx)\n",
    "    test_dataset = ArtStyleDataset(data_dir, transform=val_test_transform, mode='test', label_to_idx=label_to_idx)\n",
    "\n",
    "    # DataLoadery\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'train_dataset': train_dataset,\n",
    "        'val_dataset': val_dataset,\n",
    "        'test_dataset': test_dataset,\n",
    "        'train_transform': train_transform,\n",
    "        'val_test_transform': val_test_transform\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fdb4e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9562095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ścieżki do katalogów\n",
    "input_directory = r\"C:\\Users\\Dominik\\Desktop\\DataSet\"\n",
    "output_directory = r\"C:\\Users\\Dominik\\Desktop\\outData\"\n",
    "\n",
    "# Parametry\n",
    "target_size = (224, 224)\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "min_class_size = 2000\n",
    "max_class_size = 3000\n",
    "\n",
    "# Ścieżki wyjściowe\n",
    "processed_dir = os.path.join(output_directory, \"processed\")\n",
    "split_dir = os.path.join(output_directory, \"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4239a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\n--- ETAP 1: Preprocessing obrazów ---\")\n",
    "#preprocess_images(input_directory, processed_dir, target_size=target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "676ca6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- ETAP 2: Podział na zbiory treningowy, walidacyjny i testowy z balansowaniem klas ---\")\n",
    "# dataset_stats = split_data_with_balancing(\n",
    "#     processed_dir,\n",
    "#     split_dir,\n",
    "#     train_ratio=0.7,\n",
    "#     val_ratio=0.15,\n",
    "#     test_ratio=0.15,\n",
    "#     min_class_size=min_class_size,\n",
    "#     max_class_size=max_class_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689f4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- ETAP 3: Analiza rozkładu klas ---\")\n",
    "# plot_class_distribution(dataset_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "592be5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ETAP 4: Tworzenie DataLoaderów ---\n",
      "[train] Znaleziono 23351 obrazów w 13 klasach.\n",
      "[val] Znaleziono 5003 obrazów w 13 klasach.\n",
      "[test] Znaleziono 5007 obrazów w 13 klasach.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- ETAP 4: Tworzenie DataLoaderów ---\")\n",
    "data_loaders = create_data_loaders(\n",
    "    split_dir,\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a47b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- ETAP 5: Wizualizacja przykładów z każdej klasy ---\")\n",
    "# plot_class_examples(data_loaders['train_dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa3bbd",
   "metadata": {},
   "source": [
    "# Własna architektura CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8c2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtStyleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ArtStyleCNN, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 112x112\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 56x56\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 28x28\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # wynik 1x1x512\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350a52d",
   "metadata": {},
   "source": [
    "# Funkcja walidacyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "087e9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in tqdm(val_loader, desc=\"🧪 Walidacja\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    acc = 100. * np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"🎯 Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # 📊 Szczegółowe metryki klasyfikacji\n",
    "    class_names = [val_loader.dataset.idx_to_label[i] for i in range(len(val_loader.dataset.idx_to_label))]\n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=3))\n",
    "\n",
    "    # 📉 Macierz pomyłek\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(xticks_rotation=45, cmap=\"Blues\")\n",
    "    plt.title(\"Macierz pomyłek (Validation Set)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b684f",
   "metadata": {},
   "source": [
    "# Funkcja treningowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39cbeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "\n",
    "    train_losses, val_accuracies = [], []\n",
    "\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "\n",
    "        for i, (inputs, labels, _) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                #print(f\"Rozmiar batcha: {inputs.shape}\")\n",
    "                outputs = model(inputs)\n",
    "                #print(f\"Rozmiar outputu: {outputs.shape}\")\n",
    "                #print(f\"labels.shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                #print(f\"Loss: {loss.item()}\")  \n",
    "\n",
    "                loss.backward()\n",
    "                #print(\"Backward OK\")\n",
    "\n",
    "                optimizer.step()\n",
    "                #print(\"Optimizer step OK\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Błąd w batchu: {e}\")\n",
    "                continue\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "            # Dynamiczny update paska tqdm\n",
    "            avg_loss = running_loss / (i + 1)\n",
    "            acc = 100. * correct / total\n",
    "            pbar.set_postfix({\"Loss\": f\"{avg_loss:.4f}\", \"Train Acc\": f\"{acc:.2f}%\"})\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        \n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pt\")\n",
    "        print(f\"💾 Zapisano model po epoce {epoch+1}\")\n",
    "\n",
    "\n",
    "        print(f\"Epoch time: {time.time() - start_time:.2f}s\")\n",
    "        print(f\"Train Loss: {running_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"✔️ Zakończono epokę {epoch+1}\")\n",
    "\n",
    "    # 📈 Wykres strat i dokładności\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train Loss\", color='red')\n",
    "    plt.title(\"Train Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label=\"Validation Accuracy\", color='blue')\n",
    "    plt.title(\"Validation Accuracy Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #return train_losses, val_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023bafb",
   "metadata": {},
   "source": [
    "# Trenowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45cbbac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dane cuda:\n",
      "2.5.1\n",
      "11.8\n",
      "True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDane cuda:\")\n",
    "print(torch.__version__)                     # np. 2.6.0+cu118\n",
    "print(torch.version.cuda)                    # np. '11.8' jeśli CUDA działa\n",
    "print(torch.cuda.is_available())             # True tylko jeśli działa GPU\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff162622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba klas: 13\n",
      "Mapowanie klas: {'Academic_Art': 0, 'Art_Nouveau': 1, 'Baroque': 2, 'Expressionism': 3, 'Japanese_Art': 4, 'Neoclassicism': 5, 'Primitivism': 6, 'Realism': 7, 'Renaissance': 8, 'Rococo': 9, 'Romanticism': 10, 'Symbolism': 11, 'Western_Medieval': 12}\n",
      "Używane urządzenie: cuda\n",
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/730 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "print(\"Liczba klas:\", len(data_loaders['train_dataset'].label_to_idx))\n",
    "print(\"Mapowanie klas:\", data_loaders['train_dataset'].label_to_idx)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {device}\")\n",
    "\n",
    "num_classes = len(data_loaders['train_dataset'].label_to_idx)\n",
    "model = ArtStyleCNN(num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "try:\n",
    "    train_model(model, \n",
    "                data_loaders['train_loader'], \n",
    "                data_loaders['val_loader'], \n",
    "                criterion, optimizer, \n",
    "                device, \n",
    "                num_epochs=10)\n",
    "    print(\"✅ Trenowanie zakończone poprawnie.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Błąd w train_model: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artstyle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
