{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3c804a",
   "metadata": {},
   "source": [
    "# Libraries / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc613548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a329e80",
   "metadata": {},
   "source": [
    "# Class Artstyle\n",
    " Using Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9294e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtStyleDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, mode='train', label_to_idx=None):\n",
    "        self.data_dir = Path(data_dir) / mode\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.label_to_idx = label_to_idx  # przekazana z zewnƒÖtrz\n",
    "\n",
    "        if self.label_to_idx is None:\n",
    "            raise ValueError(\"label_to_idx musi byƒá przekazane z zewnƒÖtrz, aby zapewniƒá sp√≥jno≈õƒá klas.\")\n",
    "\n",
    "        for style_dir in self.data_dir.iterdir():\n",
    "            if style_dir.is_dir():\n",
    "                style_name = style_dir.name\n",
    "\n",
    "                if style_name not in self.label_to_idx:\n",
    "                    print(f\"‚ö†Ô∏è Uwaga: styl '{style_name}' nie znajduje siƒô w label_to_idx. Pomijam.\")\n",
    "                    continue\n",
    "\n",
    "                label_idx = self.label_to_idx[style_name]\n",
    "                for img_path in style_dir.glob('*.jpg'):\n",
    "                    self.images.append(str(img_path))\n",
    "                    self.labels.append(label_idx)\n",
    "\n",
    "        self.idx_to_label = {v: k for k, v in self.label_to_idx.items()}\n",
    "        print(f\"[{mode}] Znaleziono {len(self.images)} obraz√≥w w {len(self.label_to_idx)} klasach.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise ValueError(\"Nie mo≈ºna wczytaƒá obrazu\")\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            print(f\"B≈ÇƒÖd przetwarzania {img_path}: {e}\")\n",
    "            img = torch.zeros(3, 224, 224)\n",
    "            label = 0\n",
    "\n",
    "        return img, label, img_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0980c5f3",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4eda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(input_dir, output_dir, target_size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Przetwarza obrazy z folderu wej≈õciowego i zapisuje je w folderze wyj≈õciowym\n",
    "    z zachowaniem struktury katalog√≥w (stylu artystycznego).\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    style_dirs = [d for d in input_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    # foldery podfolderow sama nie wiem juz ile tych folderow\n",
    "    for style_dir in style_dirs:\n",
    "        style_name = style_dir.name\n",
    "        output_style_dir = output_path / style_name\n",
    "        output_style_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        sub_dirs = [d for d in style_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if sub_dirs:\n",
    "            for sub_dir in sub_dirs:\n",
    "                image_files = list(sub_dir.glob('*.jpg')) \n",
    "                print(f\"Przetwarzanie {len(image_files)} obraz√≥w w stylu {style_name}\")\n",
    "                \n",
    "                for img_path in tqdm(image_files, desc=f\"Styl: {style_name}\"):\n",
    "                    process_image(img_path, output_style_dir, target_size)\n",
    "        else:\n",
    "            image_files = list(style_dir.glob('*.jpg'))\n",
    "            print(f\"Przetwarzanie {len(image_files)} obraz√≥w w stylu {style_name}\")\n",
    "            \n",
    "            for img_path in tqdm(image_files, desc=f\"Styl: {style_name}\"):\n",
    "                process_image(img_path, output_style_dir, target_size)\n",
    "    \n",
    "    print(\"Zako≈Ñczono preprocessing obraz√≥w!\")\n",
    "\n",
    "def process_image(img_path, output_dir, target_size):\n",
    "    \"\"\"\n",
    "    Przetwarza pojedynczy obraz.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            print(f\"Nie uda≈Ço siƒô wczytaƒá: {img_path}\")\n",
    "            return\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  #BGR na RGB\n",
    "        \n",
    "        h, w = img.shape[:2]\n",
    "        aspect_ratio = w / h\n",
    "        \n",
    "        if aspect_ratio > 3 or aspect_ratio < 1/3: \n",
    "            if w > h:\n",
    "                new_w = int(h * 1.5)  \n",
    "                start_x = int((w - new_w) / 2)\n",
    "                img = img[:, start_x:start_x+new_w]\n",
    "            else:\n",
    "                new_h = int(w * 1.5) \n",
    "                start_y = int((h - new_h) / 2)\n",
    "                img = img[start_y:start_y+new_h, :]\n",
    "        \n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        if w > h:\n",
    "            new_w = min(w, target_size[0])\n",
    "            new_h = int(h * (new_w / w))\n",
    "        else:\n",
    "            new_h = min(h, target_size[1])\n",
    "            new_w = int(w * (new_h / h))\n",
    "        \n",
    "        img_resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # padding\n",
    "        padded_img = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)\n",
    "        \n",
    "        start_x = (target_size[0] - new_w) // 2\n",
    "        start_y = (target_size[1] - new_h) // 2\n",
    "        padded_img[start_y:start_y+new_h, start_x:start_x+new_w] = img_resized\n",
    "        \n",
    "        output_file = output_dir / img_path.name\n",
    "        cv2.imwrite(str(output_file), cv2.cvtColor(padded_img, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"B≈ÇƒÖd podczas przetwarzania {img_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5e358",
   "metadata": {},
   "source": [
    "# Datasplitting\n",
    "train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9097886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_with_balancing(processed_dir, output_dir, \n",
    "                             train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, \n",
    "                             min_class_size=2000, max_class_size=3000,\n",
    "                             random_state=42):\n",
    "   \n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Proporcje podzia≈Çu muszƒÖ sumowaƒá siƒô do 1.0\"\n",
    "    \n",
    "    processed_path = Path(processed_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Tymczasowy katalog dla zbalansowanych danych\n",
    "    balanced_path = output_path / \"balanced_temp\"\n",
    "    if balanced_path.exists():\n",
    "        shutil.rmtree(balanced_path)\n",
    "    balanced_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Katalogi dla zbior√≥w finalnych\n",
    "    train_dir = output_path / \"train\"\n",
    "    val_dir = output_path / \"val\"\n",
    "    test_dir = output_path / \"test\"\n",
    "    \n",
    "    for directory in [train_dir, val_dir, test_dir]:\n",
    "        if directory.exists():\n",
    "            shutil.rmtree(directory)\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    style_dirs = [d for d in processed_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(\"Rozpoczynam balansowanie klas...\")\n",
    "    \n",
    "    balanced_files = {}\n",
    "    \n",
    "    # ETAP 1: Balansowanie ka≈ºdej klasy\n",
    "    for style_dir in style_dirs:\n",
    "        style_name = style_dir.name\n",
    "        print(f\"\\nPrzetwarzanie stylu: {style_name}\")\n",
    "        \n",
    "        balanced_style_dir = balanced_path / style_name\n",
    "        balanced_style_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        image_files = list(style_dir.glob('*.jpg'))\n",
    "        total_images = len(image_files)\n",
    "        print(f\"Znaleziono {total_images} oryginalnych obraz√≥w\")\n",
    "        \n",
    "        balanced_files[style_name] = []\n",
    "        \n",
    "        # 1. Redukcja \n",
    "        if total_images > max_class_size:\n",
    "            print(f\"Redukcja liczby obraz√≥w z {total_images} do {max_class_size}\")\n",
    "            random.seed(random_state)\n",
    "            selected_files = random.sample(image_files, max_class_size)\n",
    "            \n",
    "            for src_file in selected_files:\n",
    "                dst_file = balanced_style_dir / src_file.name\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "                balanced_files[style_name].append(dst_file)\n",
    "                \n",
    "        # 2. Oversamplig dla ma≈Çych klas\n",
    "        elif total_images < min_class_size:\n",
    "            print(f\"Klasa {style_name} ma {total_images} obraz√≥w - mniej ni≈º {min_class_size}. Rozpoczynam uproszczony oversamplig.\")\n",
    "            \n",
    "            for src_file in image_files:\n",
    "                dst_file = balanced_style_dir / src_file.name\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "                balanced_files[style_name].append(dst_file)\n",
    "            \n",
    "            oversample_needed = min_class_size - total_images\n",
    "            \n",
    "            copies_per_image = oversample_needed // total_images + 1\n",
    "            \n",
    "            rotation_angles = [-5, -3, 3, 5]  # kƒÖty obrotu\n",
    "            # brightness_adjustments = [-25, -15, 15, 25] \n",
    "            \n",
    "            augmented_count = 0\n",
    "            for src_file in tqdm(image_files, desc=\"Augmentacja obraz√≥w\"):\n",
    "                for angle in rotation_angles:\n",
    "                    if augmented_count >= oversample_needed:\n",
    "                        break\n",
    "\n",
    "                    img = cv2.imread(str(src_file))\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    h, w = img.shape[:2]\n",
    "                    center = (w // 2, h // 2)\n",
    "                    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "                    rotated = cv2.warpAffine(img, rotation_matrix, (w, h))\n",
    "\n",
    "                    augmented_name = f\"{src_file.stem}_aug_r{angle}{src_file.suffix}\"\n",
    "                    augmented_path = balanced_style_dir / augmented_name\n",
    "                    cv2.imwrite(str(augmented_path), cv2.cvtColor(rotated, cv2.COLOR_RGB2BGR))\n",
    "                    balanced_files[style_name].append(augmented_path)\n",
    "                    augmented_count += 1\n",
    "\n",
    "            \n",
    "            print(f\"Utworzono {augmented_count} augmentowanych obraz√≥w\")\n",
    "            \n",
    "        # 3. Odpowiednia wielko≈õƒá (kopiuje i tyle)\n",
    "        else:\n",
    "            print(f\"Klasa {style_name} ma odpowiedniƒÖ liczbƒô obraz√≥w ({total_images}). Kopiowanie bez zmian.\")\n",
    "            for src_file in image_files:\n",
    "                dst_file = balanced_style_dir / src_file.name\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "                balanced_files[style_name].append(dst_file)\n",
    "    \n",
    "    print(\"\\nZako≈Ñczono balansowanie klas. Dzielenie na zbiory...\")\n",
    "    \n",
    "    # Stats\n",
    "    dataset_stats = {\n",
    "        'train': Counter(),\n",
    "        'val': Counter(),\n",
    "        'test': Counter()\n",
    "    }\n",
    "    \n",
    "    # ETAP 2: Podzia≈Ç na zbiory treningowy, walidacyjny i testowy\n",
    "    for style_name, files in balanced_files.items():\n",
    "        print(f\"Dzielenie klasy {style_name} na zbiory...\")\n",
    "        \n",
    "        (train_dir / style_name).mkdir(exist_ok=True)\n",
    "        (val_dir / style_name).mkdir(exist_ok=True)\n",
    "        (test_dir / style_name).mkdir(exist_ok=True)\n",
    "        \n",
    "        random.seed(random_state)\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        train_size = int(len(files) * train_ratio)\n",
    "        val_size = int(len(files) * val_ratio)\n",
    "        \n",
    "        train_files = files[:train_size]\n",
    "        val_files = files[train_size:train_size+val_size]\n",
    "        test_files = files[train_size+val_size:]\n",
    "        \n",
    "        for src_file in train_files:\n",
    "            dst_file = train_dir / style_name / src_file.name\n",
    "            shutil.copy2(src_file, dst_file)\n",
    "        \n",
    "        for src_file in val_files:\n",
    "            dst_file = val_dir / style_name / src_file.name\n",
    "            shutil.copy2(src_file, dst_file)\n",
    "        \n",
    "        for src_file in test_files:\n",
    "            dst_file = test_dir / style_name / src_file.name\n",
    "            shutil.copy2(src_file, dst_file)\n",
    "        \n",
    "        # Aktualizuj statystyki\n",
    "        dataset_stats['train'][style_name] = len(train_files)\n",
    "        dataset_stats['val'][style_name] = len(val_files)\n",
    "        dataset_stats['test'][style_name] = len(test_files)\n",
    "        \n",
    "        print(f\"  - Zbi√≥r treningowy: {dataset_stats['train'][style_name]} obraz√≥w\")\n",
    "        print(f\"  - Zbi√≥r walidacyjny: {dataset_stats['val'][style_name]} obraz√≥w\")\n",
    "        print(f\"  - Zbi√≥r testowy: {dataset_stats['test'][style_name]} obraz√≥w\")\n",
    "    \n",
    "    shutil.rmtree(balanced_path)\n",
    "    \n",
    "    print(\"\\nZako≈Ñczono podzia≈Ç danych na zbiory!\")\n",
    "    return dataset_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d28af",
   "metadata": {},
   "source": [
    "# Data distribution between categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989134ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(dataset_stats):\n",
    "    \"\"\"\n",
    "    Tworzy wykresy ko≈Çowe przedstawiajƒÖce rozk≈Çad klas w ka≈ºdym ze zbior√≥w.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, dataset_type in enumerate(['train', 'val', 'test']):\n",
    "        stats = dataset_stats[dataset_type]\n",
    "        labels = list(stats.keys())\n",
    "        values = list(stats.values())\n",
    "        \n",
    "        axs[i].pie(values, labels=None, autopct='%1.1f%%', startangle=90)\n",
    "        axs[i].set_title(f'Rozk≈Çad klas w zbiorze {dataset_type}')\n",
    "    \n",
    "    # Dodaj wsp√≥lnƒÖ legendƒô\n",
    "    fig.legend(labels, loc='lower center', ncol=min(5, len(labels)))\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Tworzymy te≈º wykres s≈Çupkowy dla lepszego por√≥wnania\n",
    "    df = pd.DataFrame(dataset_stats)\n",
    "    df.plot(kind='bar', figsize=(12, 6))\n",
    "    plt.title('Liczba obraz√≥w w ka≈ºdej klasie wed≈Çug zbioru')\n",
    "    plt.xlabel('Klasa')\n",
    "    plt.ylabel('Liczba obraz√≥w')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distribution_bar.png')\n",
    "    plt.show()\n",
    "\n",
    "def denormalize(img_tensor, mean, std):\n",
    "    \"\"\"\n",
    "    Odwraca normalizacjƒô obrazu (tensor PyTorch) do zakresu [0,1].\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "    std = torch.tensor(std).view(-1, 1, 1)\n",
    "    return img_tensor * std + mean\n",
    "\n",
    "def plot_class_examples(dataset, num_examples=5):\n",
    "    \"\"\"\n",
    "    Wy≈õwietla przyk≈Çadowe obrazy z ka≈ºdej klasy z oryginalnymi kolorami.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Grupuj indeksy wed≈Çug klas\n",
    "    class_indices = {}\n",
    "    for idx, (_, label, _) in enumerate(dataset):\n",
    "        if label not in class_indices:\n",
    "            class_indices[label] = []\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Dla ka≈ºdej klasy wy≈õwietl przyk≈Çady\n",
    "    for label, indices in class_indices.items():\n",
    "        class_name = dataset.idx_to_label[label]\n",
    "        \n",
    "        sample_indices = random.sample(indices, min(num_examples, len(indices)))\n",
    "        \n",
    "        fig, axs = plt.subplots(1, len(sample_indices), figsize=(15, 3))\n",
    "        fig.suptitle(f'Przyk≈Çady klasy: {class_name}')\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            img, _, img_path = dataset[idx]\n",
    "            \n",
    "            # Konwersja z PyTorch tensor do formatu odpowiedniego dla matplotlib\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                if img.dim() == 3 and img.shape[0] in [1, 3, 4]:  # je≈õli [C, H, W]\n",
    "                    img = denormalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "                    img = np.clip(img, 0, 1)  # ≈ºeby warto≈õci nie wysz≈Çy poza zakres [0, 1]\n",
    "                    \n",
    "                    if img.shape[2] == 1:\n",
    "                        img = img.squeeze(2)\n",
    "                else:\n",
    "                    img = img.cpu().numpy()\n",
    "            \n",
    "            if len(sample_indices) == 1:\n",
    "                axs.imshow(img)\n",
    "                axs.set_title(Path(img_path).name)\n",
    "                axs.axis('off')\n",
    "            else:\n",
    "                axs[i].imshow(img)\n",
    "                axs[i].set_title(Path(img_path).name)\n",
    "                axs[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd87b84",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2c5ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(data_dir, batch_size=32, img_size=224, num_workers=4):\n",
    "    def get_label_to_idx(data_dir):\n",
    "        train_dir = Path(data_dir) / 'train'\n",
    "        labels = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
    "        return {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    # Sp√≥jna mapa etykiet\n",
    "    label_to_idx = get_label_to_idx(data_dir)\n",
    "\n",
    "    # Transformacje\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(img_size + 32),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(img_size + 32),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Datasety z tƒÖ samƒÖ mapƒÖ etykiet\n",
    "    train_dataset = ArtStyleDataset(data_dir, transform=train_transform, mode='train', label_to_idx=label_to_idx)\n",
    "    val_dataset = ArtStyleDataset(data_dir, transform=val_test_transform, mode='val', label_to_idx=label_to_idx)\n",
    "    test_dataset = ArtStyleDataset(data_dir, transform=val_test_transform, mode='test', label_to_idx=label_to_idx)\n",
    "\n",
    "    # DataLoadery\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'train_dataset': train_dataset,\n",
    "        'val_dataset': val_dataset,\n",
    "        'test_dataset': test_dataset,\n",
    "        'train_transform': train_transform,\n",
    "        'val_test_transform': val_test_transform\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fdb4e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9562095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ≈öcie≈ºki do katalog√≥w\n",
    "input_directory = r\"C:\\Users\\Dominik\\Desktop\\DataSet\"\n",
    "output_directory = r\"C:\\Users\\Dominik\\Desktop\\outData\"\n",
    "\n",
    "# Parametry\n",
    "target_size = (224, 224)\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "min_class_size = 2000\n",
    "max_class_size = 3000\n",
    "\n",
    "# ≈öcie≈ºki wyj≈õciowe\n",
    "processed_dir = os.path.join(output_directory, \"processed\")\n",
    "split_dir = os.path.join(output_directory, \"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4239a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\n--- ETAP 1: Preprocessing obraz√≥w ---\")\n",
    "#preprocess_images(input_directory, processed_dir, target_size=target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "676ca6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- ETAP 2: Podzia≈Ç na zbiory treningowy, walidacyjny i testowy z balansowaniem klas ---\")\n",
    "# dataset_stats = split_data_with_balancing(\n",
    "#     processed_dir,\n",
    "#     split_dir,\n",
    "#     train_ratio=0.7,\n",
    "#     val_ratio=0.15,\n",
    "#     test_ratio=0.15,\n",
    "#     min_class_size=min_class_size,\n",
    "#     max_class_size=max_class_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689f4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- ETAP 3: Analiza rozk≈Çadu klas ---\")\n",
    "# plot_class_distribution(dataset_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "592be5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ETAP 4: Tworzenie DataLoader√≥w ---\n",
      "[train] Znaleziono 23351 obraz√≥w w 13 klasach.\n",
      "[val] Znaleziono 5003 obraz√≥w w 13 klasach.\n",
      "[test] Znaleziono 5007 obraz√≥w w 13 klasach.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- ETAP 4: Tworzenie DataLoader√≥w ---\")\n",
    "data_loaders = create_data_loaders(\n",
    "    split_dir,\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a47b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- ETAP 5: Wizualizacja przyk≈Çad√≥w z ka≈ºdej klasy ---\")\n",
    "# plot_class_examples(data_loaders['train_dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa3bbd",
   "metadata": {},
   "source": [
    "# W≈Çasna architektura CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8c2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtStyleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ArtStyleCNN, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 112x112\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 56x56\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 28x28\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # wynik 1x1x512\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350a52d",
   "metadata": {},
   "source": [
    "# Funkcja walidacyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "087e9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in tqdm(val_loader, desc=\"üß™ Walidacja\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    acc = 100. * np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"üéØ Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # üìä Szczeg√≥≈Çowe metryki klasyfikacji\n",
    "    class_names = [val_loader.dataset.idx_to_label[i] for i in range(len(val_loader.dataset.idx_to_label))]\n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=3))\n",
    "\n",
    "    # üìâ Macierz pomy≈Çek\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(xticks_rotation=45, cmap=\"Blues\")\n",
    "    plt.title(\"Macierz pomy≈Çek (Validation Set)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b684f",
   "metadata": {},
   "source": [
    "# Funkcja treningowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39cbeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "\n",
    "    train_losses, val_accuracies = [], []\n",
    "\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "\n",
    "        for i, (inputs, labels, _) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                #print(f\"Rozmiar batcha: {inputs.shape}\")\n",
    "                outputs = model(inputs)\n",
    "                #print(f\"Rozmiar outputu: {outputs.shape}\")\n",
    "                #print(f\"labels.shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                #print(f\"Loss: {loss.item()}\")  \n",
    "\n",
    "                loss.backward()\n",
    "                #print(\"Backward OK\")\n",
    "\n",
    "                optimizer.step()\n",
    "                #print(\"Optimizer step OK\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"B≈ÇƒÖd w batchu: {e}\")\n",
    "                continue\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "            # Dynamiczny update paska tqdm\n",
    "            avg_loss = running_loss / (i + 1)\n",
    "            acc = 100. * correct / total\n",
    "            pbar.set_postfix({\"Loss\": f\"{avg_loss:.4f}\", \"Train Acc\": f\"{acc:.2f}%\"})\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        \n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pt\")\n",
    "        print(f\"üíæ Zapisano model po epoce {epoch+1}\")\n",
    "\n",
    "\n",
    "        print(f\"Epoch time: {time.time() - start_time:.2f}s\")\n",
    "        print(f\"Train Loss: {running_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"‚úîÔ∏è Zako≈Ñczono epokƒô {epoch+1}\")\n",
    "\n",
    "    # üìà Wykres strat i dok≈Çadno≈õci\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train Loss\", color='red')\n",
    "    plt.title(\"Train Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label=\"Validation Accuracy\", color='blue')\n",
    "    plt.title(\"Validation Accuracy Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #return train_losses, val_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023bafb",
   "metadata": {},
   "source": [
    "# Trenowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45cbbac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dane cuda:\n",
      "2.5.1\n",
      "11.8\n",
      "True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDane cuda:\")\n",
    "print(torch.__version__)                     # np. 2.6.0+cu118\n",
    "print(torch.version.cuda)                    # np. '11.8' je≈õli CUDA dzia≈Ça\n",
    "print(torch.cuda.is_available())             # True tylko je≈õli dzia≈Ça GPU\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff162622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba klas: 13\n",
      "Mapowanie klas: {'Academic_Art': 0, 'Art_Nouveau': 1, 'Baroque': 2, 'Expressionism': 3, 'Japanese_Art': 4, 'Neoclassicism': 5, 'Primitivism': 6, 'Realism': 7, 'Renaissance': 8, 'Rococo': 9, 'Romanticism': 10, 'Symbolism': 11, 'Western_Medieval': 12}\n",
      "U≈ºywane urzƒÖdzenie: cuda\n",
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/730 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "print(\"Liczba klas:\", len(data_loaders['train_dataset'].label_to_idx))\n",
    "print(\"Mapowanie klas:\", data_loaders['train_dataset'].label_to_idx)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"U≈ºywane urzƒÖdzenie: {device}\")\n",
    "\n",
    "num_classes = len(data_loaders['train_dataset'].label_to_idx)\n",
    "model = ArtStyleCNN(num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "try:\n",
    "    train_model(model, \n",
    "                data_loaders['train_loader'], \n",
    "                data_loaders['val_loader'], \n",
    "                criterion, optimizer, \n",
    "                device, \n",
    "                num_epochs=10)\n",
    "    print(\"‚úÖ Trenowanie zako≈Ñczone poprawnie.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå B≈ÇƒÖd w train_model: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artstyle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
